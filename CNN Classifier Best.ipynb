{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 CNN\n",
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import torchvision\n",
    "import torchsummary\n",
    "import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU: GTX 1660 Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current Device: 0\n",
      "GPU name is NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if we are using GPU as device, and later we can train using my GTX 1660 Ti\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "devNumber = torch.cuda.current_device()\n",
    "print(f\"Current Device: {devNumber}\")\n",
    "devName = torch.cuda.get_device_name(devNumber)\n",
    "print(f\"GPU name is {devName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the CIFAR10 Dataset from Torchvision\n",
    "Hyperparameters to tune:\n",
    "1. Batch size:\n",
    "    Change the number of batches, 8, 16, 32, 64, 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Convert original CIFAR10 images to Pytorch tensors and normalize the pixels\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# HYPERPARAMETER: Batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Create the training set and data loader to handle batches\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "# Create the test set\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "# Class labels for CIFAR10\n",
    "classes = (\n",
    "    'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape and Show the CIFAR10 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "\t Samples: (50000, 32, 32, 3)\n",
      "\t Labels: 50000\n",
      "Test Set:\n",
      "\t Samples: (10000, 32, 32, 3)\n",
      "\t Labels: 10000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'imshow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Lets see the images\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mshow_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Print the label for the image\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_set\u001b[38;5;241m.\u001b[39mclasses[label]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels))\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mshow_image\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     12\u001b[0m npimg \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Change dimensions for display for matplotlib height width channel\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m(np\u001b[38;5;241m.\u001b[39mtranspose(npimg, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)))  \n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32md:\\Programming\\Python\\CMSC 478\\HW7\\venv\\lib\\site-packages\\matplotlib\\_api\\__init__.py:217\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'imshow'"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\")\n",
    "print(f\"\\t Samples: {train_set.data.shape}\")\n",
    "print(f\"\\t Labels: {len(train_set.targets)}\")\n",
    "print('Test Set:')\n",
    "print(f\"\\t Samples: {test_set.data.shape}\")\n",
    "print(f\"\\t Labels: {len(test_set.targets)}\")\n",
    "\n",
    "def show_image(img):\n",
    "    # Unnormalize the image\n",
    "    img = img / 2 + 0.5\n",
    "    # Convert the tensor to a numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Change dimensions for display for matplotlib height width channel\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a batch of images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Lets see the images\n",
    "show_image(torchvision.utils.make_grid(images))\n",
    "\n",
    "# Print the label for the image\n",
    "print(' '.join(f'{train_set.classes[label]}' for label in labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "Hyperparameters to tune:\n",
    "1. Number of Filters:\n",
    "    Change the number of output channels in nn.Conv2d (e.g., from 6 to 12 or 16).\n",
    "2. Kernel Size:\n",
    "    Adjust the size of the convolution kernel (e.g., from 5x5 to 3x3 or 7x7).\n",
    "3. Stride:\n",
    "    Change the step size for sliding the kernel over the image (e.g., stride=1 or stride=2).\n",
    "4. Padding:\n",
    "    Add padding to preserve the spatial dimensions (e.g., padding=1).\n",
    "5. Pool Size:\n",
    "    Change the size of the pooling window (e.g., from 2x2 to 3x3).\n",
    "6. Pooling Type:\n",
    "    Try different pooling layers like nn.AvgPool2d instead of nn.MaxPool2d.\n",
    "7. Stride in Pooling:\n",
    "    Adjust the stride (e.g., stride=1 or stride=2).\n",
    "8. Number of Neurons:\n",
    "    Increase or decrease the number of neurons in fc1 and fc2 (e.g., 120 to 256 or 84 to 128).\n",
    "9. Number of Layers:\n",
    "    Add or remove fully connected layers to increase or decrease model capacity.\n",
    "10. Different Activation Functions:\n",
    "    Use relu, sigmoid, tanh, elu\n",
    "11. Dropout and Rate:\n",
    "    Add dropout and rates to layers to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula for Convolution Layer Shape (Assume Square)\n",
    "$$K: \\text{Kernel Size (square)}$$\n",
    "$$S: \\text{Stride Size (move by)}$$\n",
    "$$P: \\text{Padding Size (edges)}$$\n",
    "$$Shape=floor(\\frac{H-K+2P}{S})+1$$\n",
    "### Formula for Pooling Layer Shape (Assume Square)\n",
    "$$Shape=floor(\\frac{H-K}{S})+1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Output Size: 28\n",
      "Pooling Output Size: 14\n"
     ]
    }
   ],
   "source": [
    "def conv_output_shape(input_size, kernel_size, stride=1, padding=0):\n",
    "    return (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "def pool_output_shape(input_size, kernel_size, stride=None):\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    return (input_size - kernel_size) // stride + 1\n",
    "\n",
    "example = conv_output_shape(input_size=32, kernel_size=5, stride=1, padding=0)\n",
    "print(f\"Convolution Output Size: {example}\")\n",
    "print(f\"Pooling Output Size: {pool_output_shape(example, kernel_size=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the sequential method from NN in order to keep everything more organized\n",
    "class ConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional layers detect patterns like edges, textures, and shapes in the image\n",
    "        self.convolution_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Pooling reduces the size of the feature maps, decreasing computational cost and helping the network focus on the most important features\n",
    "        self.convolution_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convolution_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convolution_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.convolution_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        # self.fc_1 = nn.Sequential(\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(512 * 4 * 4, 120),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        # self.fc_2 = nn.Sequential(\n",
    "        #     nn.Linear(120, 84),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        self.fc_classify = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convolution_1(x)\n",
    "        x = self.convolution_2(x)\n",
    "        x = self.convolution_3(x)\n",
    "        x = self.convolution_4(x)\n",
    "        x = self.convolution_5(x)\n",
    "        # x = self.fc_1(x)\n",
    "        # x = self.fc_2(x)\n",
    "        x = self.fc_classify(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer\n",
    "Hyperparameters to tune:\n",
    "1. Optimizer:\n",
    "    Try Adam, Adagrad\n",
    "2. Adjust Learning Rate:\n",
    "    Try .005, .0001, .0005, .01, .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [32, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [32, 64, 32, 32]             128\n",
      "              ReLU-3           [32, 64, 32, 32]               0\n",
      "            Conv2d-4          [32, 128, 32, 32]          73,856\n",
      "       BatchNorm2d-5          [32, 128, 32, 32]             256\n",
      "              ReLU-6          [32, 128, 32, 32]               0\n",
      "         MaxPool2d-7          [32, 128, 16, 16]               0\n",
      "            Conv2d-8          [32, 256, 16, 16]         295,168\n",
      "       BatchNorm2d-9          [32, 256, 16, 16]             512\n",
      "             ReLU-10          [32, 256, 16, 16]               0\n",
      "        MaxPool2d-11            [32, 256, 8, 8]               0\n",
      "           Conv2d-12            [32, 512, 8, 8]       1,180,160\n",
      "      BatchNorm2d-13            [32, 512, 8, 8]           1,024\n",
      "             ReLU-14            [32, 512, 8, 8]               0\n",
      "        MaxPool2d-15            [32, 512, 4, 4]               0\n",
      "           Conv2d-16            [32, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-17            [32, 512, 4, 4]           1,024\n",
      "             ReLU-18            [32, 512, 4, 4]               0\n",
      "           Conv2d-19            [32, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-20            [32, 512, 4, 4]           1,024\n",
      "             ReLU-21            [32, 512, 4, 4]               0\n",
      "        MaxPool2d-22            [32, 512, 1, 1]               0\n",
      "          Flatten-23                  [32, 512]               0\n",
      "           Linear-24                   [32, 10]           5,130\n",
      "================================================================\n",
      "Total params: 6,279,690\n",
      "Trainable params: 6,279,690\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 242.25\n",
      "Params size (MB): 23.96\n",
      "Estimated Total Size (MB): 266.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Make a CNN object using the GPU\n",
    "cnn = ConvNN().to(device)\n",
    "# Get loss function and optimizer for it\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# Lets see some params\n",
    "torchsummary.summary(cnn, (3, 32, 32), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Convolutional Neural Network\n",
    "Hyperparameters to tune:\n",
    "1. Epochs\n",
    "    Try 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  16%|█▌        | 2015/12500 [00:31<01:54, 91.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  24%|██▍       | 3002/12500 [00:42<01:43, 91.90it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n",
    "        # data is a list [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Zero out the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward propogate\n",
    "        outputs = cnn(inputs)\n",
    "        # Get loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward propogate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Record loss\n",
    "        train_loss += loss.item() \n",
    "         \n",
    "        # Get Stats after every 2000 mini-batches, or 2000 * batch_size inputs\n",
    "        if i % 2000 == 1999:    \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {train_loss / 2000:.3f}')\n",
    "            train_loss = 0.0\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "PATH = \"./cifar_cnn.pth\"\n",
    "torch.save(cnn.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_1 = ConvNN().to(device)\n",
    "cnn_1.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy using test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, testloader):\n",
    "    num_correct = 0\n",
    "    # Not training, only testing, so no gradient\n",
    "    with torch.no_grad():\n",
    "        # We are in evaluation mode using our CNN\n",
    "        net.eval()\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # classify our test images using our CNN\n",
    "            outputs = net(images).to(device)\n",
    "            # the class with the highest value is what we choose as prediction\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            # Check if labels match the predicted and sum up all correct and make it an int\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "    # Return the percentage correctly classified\n",
    "    percent_correct = num_correct / len(testloader.dataset)\n",
    "    percent_correct = percent_correct * 100\n",
    "    print(f\"Accuracy for entire test set: {percent_correct:.2f} %\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def test_accuracy_per_class(net, testloader):\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images).to(device)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "    # print accuracy for each class\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f\"Accuracy for class: {classname:5s} is {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see the accuracy on test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for entire test set: 79.49 %\n",
      "Accuracy for class: plane is 78.00 %\n",
      "Accuracy for class: car   is 93.50 %\n",
      "Accuracy for class: bird  is 75.60 %\n",
      "Accuracy for class: cat   is 69.70 %\n",
      "Accuracy for class: deer  is 73.10 %\n",
      "Accuracy for class: dog   is 68.30 %\n",
      "Accuracy for class: frog  is 88.90 %\n",
      "Accuracy for class: horse is 73.40 %\n",
      "Accuracy for class: ship  is 87.30 %\n",
      "Accuracy for class: truck is 87.10 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(cnn_1, test_loader)\n",
    "test_accuracy_per_class(cnn_1, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
